import requests
from bs4 import BeautifulSoup
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def fetch_page(url):
    """Fetch page content from URL."""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logging.error(f"Error fetching {url}: {str(e)}")
        return None

def build_website_map():
    """Build a static website map."""
    return {
        'car accidents': {'url': 'https://stolmeierlaw.com/car-accidents/', 'subcategories': {}},
        'medical malpractice': {'url': 'https://stolmeierlaw.com/medical-malpractice/', 'subcategories': {}},
        'slip, trip or fall': {'url': 'https://stolmeierlaw.com/slip,-trip-or-fall/', 'subcategories': {}},
        'truck accidents': {'url': 'https://stolmeierlaw.com/truck-accidents/', 'subcategories': {}},
        '18-wheeler accidents': {'url': 'https://stolmeierlaw.com/18-wheeler-accidents/', 'subcategories': {}},
        'motorcycle accidents': {'url': 'https://stolmeierlaw.com/motorcycle-accidents/', 'subcategories': {}},
        'dog bites & attacks': {'url': 'https://stolmeierlaw.com/dog-bites-attacks/', 'subcategories': {}},
        'product liability': {'url': 'https://stolmeierlaw.com/product-liability/', 'subcategories': {}},
        'wrongful death': {'url': 'https://stolmeierlaw.com/', 'subcategories': {}},
        'recent results': {'url': 'https://stolmeierlaw.com/recent-results/', 'subcategories': {}},
        'about': {'url': 'https://stolmeierlaw.com/about/', 'subcategories': {}},
        'contact us': {'url': 'https://stolmeierlaw.com/contact-us/', 'subcategories': {}}
    }

def scrape_contact_info_fallback():
    """Fallback for contact info."""
    return "Address: 219 E. Craig Place, San Antonio, TX 78212; Phone: 210-227-3612; Email: info@stolmeierlaw.com"

def scrape_targeted_content(keywords, content_type, session_id, url, website_map):
    """Scrape targeted content with strict filtering."""
    logging.info(f"Scraping: keywords={keywords}, content_type={content_type}, url={url}")
    
    if not url:
        logging.warning(f"No URL provided for keywords={keywords}")
        return "Sorry, I couldn’t fetch the content for this section."
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Remove irrelevant elements
        for element in soup.find_all(['nav', 'footer', 'div', 'ul', 'ol'], class_=['menu', 'dropdown-menu', 'footer', 'sidebar', 'widget', 'related-posts']):
            element.decompose()
        for element in soup.find_all(['script', 'style', 'form', 'iframe']):
            element.decompose()

        if content_type == "causes" and "Car Accidents" in keywords:
            causes_section = None
            for ul in soup.find_all('ul'):  # Line 59 - Fixed syntax
                li_texts = [li.get_text().strip().lower() for li in ul.find_all('li')]
                if any(text in li_texts for text in ['speeding', 'impaired driving', 'distracted driving']):
                    causes_section = ul
                    break
            
            if causes_section:
                causes = []
                current_category = None
                for li in causes_section.find_all('li', recursive=False):
                    text = li.get_text().strip()
                    if text in ['Speeding', 'Impaired Driving', 'Reckless Driving', 'Distracted Driving']:
                        current_category = text
                        if text == 'Speeding':
                            causes.append(text)
                    elif current_category and text in [
                        'Drunk driving', 'Driving under the influence of narcotics', 'Sleep deprivation',
                        'Disregarding traffic signs and/or signals', 'Disregarding traffic lanes', 'Disregarding fellow motorists',
                        'Texting', 'Non-hands free phone calls', 'Eating', 'Changing the radio', 'Use of GPS'
                    ]:
                        causes.append(f"{current_category}: {text}")
                
                if causes:
                    logging.info(f"Scraped causes: {causes[:3]}...")
                    return "; ".join(causes)
            
            logging.warning(f"No causes section found on {url}")
            return "Sorry, I couldn’t fetch the causes for Car Accidents."

        elif content_type == "description":
            main_content = soup.find('div', class_='entry-content') or soup.find('main') or soup.find('div', class_='content-area')
            if main_content:
                # Remove causes list from description
                for ul in main_content.find_all('ul'):
                    li_texts = [li.get_text().strip().lower() for li in ul.find_all('li')]
                    if any(text in li_texts for text in ['speeding', 'impaired driving']):
                        ul.decompose()
                text = main_content.get_text(separator=' ', strip=True)
                return text[:150] if text else "No description available."
            return "Sorry, I couldn’t fetch the description for this section."

        elif content_type == "contact":
            contact_section = soup.find('div', class_='contact-info') or soup.find('section', class_='contact')
            if contact_section:
                contact_text = contact_section.get_text(separator=' ', strip=True).strip()
                if len(contact_text) > 50:
                    return contact_text[:150]
                logging.warning(f"Contact text too short: {contact_text}")
            return scrape_contact_info_fallback()

        elif content_type == "general":
            for element in soup.find_all('p'):
                text = element.get_text().strip().lower()
                if any(keyword.lower() in text for keyword in keywords):
                    return element.get_text(separator=' ', strip=True)[:150].strip()
            logging.warning(f"No general content found for keywords={keywords}")
            return "Sorry, I couldn’t find relevant information."

        logging.warning(f"Unknown content_type: {content_type}")
        return "Sorry, I couldn’t fetch the content for this section."
    except Exception as e:
        logging.error(f"Scraping error for {url}: {str(e)}")
        return "Sorry, I couldn’t fetch the content for this section."